# on the sins of short-form development
#dissertation #short-form 

source: Smith, G. T., McCarthy, D. M., & Anderson, K. G. (2000). On the sins of short-form development. _Psychological assessment_, _12_(1), 102.

# debate: are short forms good?
1. stance 1: no. you don't use short forms 
	1. Levy, P. (1968). Short-form tests: A methodological review. _Psychological Bulletin_, _69_(6), 410. ^1d478b
2. stance 2: yes, but it's not done well. the authors support this.

# Some inspirations from this
1. so short-form development is usually item-selection and then validation (sometimes, before the development, a priori estimates of reliability, validity and stuff need to be computed)
	1. we are providing a new approach/ or we are redefining it
		1. the second step is now "prediction" using neural networks
		2. the validity structure of CMPA is different from other tests, because of the "construct" it defines, 
	2. how does that apply to [[CMPA]]?
2. should we adopt part of the recommendations here and provide a priori estimates of relaibility and stuff and show how NN exceeds our expectations?

3. they used the keywords “abbreviated,” “abridged,” or “shortened,” or were referred to as  “short form", so maybe I should be looking at least in machine learning
	1. maybe say we followed their strategy and tried to look for these keywords
4. 
# Useful points for me
1. not all forms are surveys
	1. some are observation reports
	2. behavioural tests
2. short-form development is common in psychology and was maybe first introduced in 1918
3. the purpose was
	1. screening (ours is not screening?)
	2. diagnosis (ours is diagnosis?)
	3. adapt the form for children
4. the problems were
	1. some stupid researchers assumed that the short form would be just as valid as the new form
5. the principle was
	1. to think of short form development as essentially developing a new form
	2. but the validity evidence can come from the original form
6. 

# 2 general sins
1. researchers assume that all of the reliability validity and reliability evidence automatically apply to the original scale
2. the second is that because the new form is shorter, less validity evidence is needed
	1. ok... well... is this still the case?

# 9 specific sins
1. when your long form wasn't well validated in the first place
2. fails to show that your short form preserves the content coverage of each factor in the measure
3. fails to show that your short form measures each factor scale reliably
4. fails to show that your short form has adequate overlapping variance with the full form, using independent administrations
5. fails to show empirically that your short form reproduces the factor structure of a multifactoral instrument
6. if your short form omits subfactors and preserves only overall factors, then fail to show that the short form preservers the content domains represented by the subfactors
7. fail to show that each factor in the short form has validity on an independent sample
8. **fails to show that classification rates remain high with the short form** (maybe the most relevant to us)
9. fails to show that your short form offers meaningful time or resource savings for the loss in validity

# Recommendations
1. ensure the parent measure has been sufficiently validated for the intended purpose
2. clarify the intended use of the short form (screening measure vs diagnosis measure)
3. compute an a priori estimate of the short form's reliability (using CTT?)
4. compute an a priori estimate of the likely overlap between the short form and its parent
5. compute an a priori estimate of validity correlations of your short form with key criteria
6. where appropriate, compute an a priori estimate of the classification accuracy (whether hit rates or elimination of false negatives)
	1. ** oh, this may be related to the part when we talk about [[Performance Metrics for Dichotomous Outcomes]]
7. compute a priori estimates of the time saved and the validity lost




# Future directions
1. tailored testing and IRT based approaches