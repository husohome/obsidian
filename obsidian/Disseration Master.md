# Dissertation Master
#dissertation #amery #to-do 

# Chapter 1
nothing yet
# Chapter 2
nothing yet

# Chapter 3
## Amery's advice on Chapter 3
1. restructure
2. make standalone
3. 5C
4. the test shortening has already been done
	1. we've selected items (all the Likert)
	2. it's about making the shortened test predict just as well as the long form
	3. may cut Neural network intros
	4. may increase coverage of hyperparameter tuning
5. cut down to 35 pages or below

# Other stuff
7. BB-class (traditional psychometric -- CTT, IRT, approach to classification, based on a cut score)

## Introduction/ Justification

## Scoring Short-forms
2. Short forms (do not talk about short form development at all)
3. 	1. are used for 
		2. efficiency
		3. burden on respondents
		4. adaptation for children
	5. even if the researchers have considered  content cut-off or reliability/validity issues, there is usually a sacrifice in the performance in the short-form (consistency between the long-form and short-form results)
		1. cite papers calculate the correlations/ consistency in classification between long form and short form
		2. **quick systematic review on (it's ok to spend some space here)
			1. **classification performance**
			2. **simple score correlation between short and long form**
			3. can give real examples
	6. how do people usually use the score?
		1. short-form:
			1. total (subdimension scores)
			2. factor scores
## Why Neural Networks (for scoring the short form)
1. NN, machine learning in general, allows you to use different scoring methods for different target performance (test use!), e.g. 
	1. minimizing false positives/ negatives (~but CTT, factor analysis and IRT can't)
		1. board examiner of medical doctors you are more afraid of false positives
		2. immigration lawyers -- you would be more concerns about false negatives
2. it has state-of-the-art performance in prediction/classification tasks
3. borrowing from machine learning, maybe there's a way to score the the short form such that it performs as well as the long form with very little sacrifice
4. locate neural network, a bit of methods

## The CMPA Short-form (as an example)
1. [[CMPA- overview]] <- treat CMPA as "they"
2. CMPA decided on a short-form, only the Likerts, because...
4. however, it may be difficult to use traditional scoring approach to  identify repsondents' top 3 preferences (the original purpose)
	1. because too many categories
5. talk about CMPA 
6. why is CMPA an interesting case 


# Methods and Results
1. baselines
	1. add up Likert-score scores by major
	2. factor scores (say you can't, there are 33 dimensions) -> and compare 
	3. use linear discriminant 


# Chapter 4
# Chapter 5
# Some Useful references

[[okewu - systematic review on the use of ANN in educational data mining]]


#  Links to Idea Snippets
[[Neural Network Links]]
[[CMPA]]



# Other Advice
[[Amery's Advice]]





# Haven't organized


Donnellan, M. B., Oswald, F. L., Baird, B. M., & Lucas, R. E. (2006). The mini-IPIP scales: tiny-yet-effective measures of the Big Five factors of personality. Psychological assessment, 18(2), 192.

Rammstedt, B., & John, O. P. (2007). Measuring personality in one minute or less: A 10-item short version of the Big Five Inventory in English and German. Journal of research in Personality, 41(1), 203-212.

Raes, F., Pommier, E., Neff, K. D., & Van Gucht, D. (2011). Construction and factorial validation of a short form of the self‐compassion scale. Clinical psychology & psychotherapy, 18(3), 250-255.

Paap, M. C., Meijer, R. R., Van Bebber, J., Pedersen, G., Karterud, S., Hellem, F. M., & Haraldsen, I. R. (2011). A study of the dimensionality and measurement precision of the SCL‐90‐R using item response theory. International journal of methods in psychiatric research, 20(3), e39-e55.

Hastie, T., Tibshirani, R., & Friedman, J. (2009). Overview of supervised learning. In The elements of statistical learning (pp. 9-41). Springer, New York, NY.

Zaremba, W., Sutskever, I., & Vinyals, O. (2014). Zaremba, W., Sutskever, I., & Vinyals, O. (2014). Recurrent neural network regularization. arXiv preprint arXiv:1409.2329.

Murugan, P., & Durairaj, S. (2017). Regularization and optimization strategies in deep convolutional neural network. arXiv preprint arXiv:1712.04711.

Baykan, N. A., & Yılmaz, N. (2011). A mineral classification system with multiple artificial neural network using k-fold cross validation. Mathematical and Computational Applications, 16(1), 22-30.

Cui, H., & Bai, J. (2019). A new hyperparameters optimization method for convolutional neural networks. Pattern Recognition Letters, 125, 828-834.

Salloum, S. A., Alshurideh, M., Elnagar, A., & Shaalan, K. (2020, April). Mining in Educational Data: Review and Future Directions. In AICV (pp. 92-102).

Pontes, F. J., Amorim, G. F., Balestrassi, P. P., Paiva, A. P., & Ferreira, J. R. (2016). Design of experiments and focused grid search for neural network parameter optimization. Neurocomputing, 186, 22-34.

Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of machine learning research, 13(2).

Xiao, X., Yan, M., Basodi, S., Ji, C., & Pan, Y. (2020). Efficient hyperparameter optimization in deep learning using a variable length genetic algorithm. arXiv preprint arXiv:2006.12703.

# Dumps


[[duimps]]