              

# Performance Metrics for Dichotomous Outcomes

#dissertation 

**Accuracy.** Accuracy, as the most intuitive of all, quantifies the overall correct guesses by the model. However, it is hard to trace if the correct guesses are from true positives (when the real answer is yes, and the model says yes too) or true negatives (when the real answer is no and the model says no too). In the case of CMPA, this inability can be very inconvenient. If a major, for example, electronic engineering, is not popular in the first place, having only 0.2% of the respondents with it in their top 3, then the probability of “no”, as the real answer is 99.8%. This means if the model blindly guesses “no” without even looking at the Likert-type items associated with it, it will end up with a misleadingly high accuracy of 99.8%, all of which comes from true negatives and zero of it comes from true positives. If the model does not take the information of Likert-type items in the first place, then it is not shortening an assessment; it simply learned a heuristic of always predicting that no one likes electronic engineering. In the case of CMPA, given the purpose is to help respondents identify their preferences for majors, true positives are a lot more important than true negatives. Thus, more fine-grained metrics that focus on true positives should be used, which lead us to Recall, Precision, and F1, introduced next.

**Recall/ Sensitivity.** In the literature, it is also common to report another two metrics: recall and precision. In the context of CMPA, recall is the probability that if a student really likes a major in their top 3, the model says yes too. Like accuracy, it has the same problem of not being able to catch blind guessing and is heavily influenced by response set (see Appendix A). The more the model is trained to say yes, no matter if the yes-es are related to the real data, recall is going to look high; if the model always says yes to all majors, recall is by definition 100%. If the shortened version of CMPA were to predict all majors as in the participants’ top 3, it is pretty much useless, but it will have perfect recall. That said, given that the model is not blindly guessing, it is still important to know if the model is sensitive enough to “preference”, such that if a student really has a major in their top 3, that the model can detect it by saying yes. Hence, we preferred F1-gain and Precision-gain, introduced later, which incorporates some information from recall, but not directly recall.

**Precision/ Positive Predictive Value.** Precision, on the other hand, is the probability that if the model says the student likes a major, the student really likes the major in the data. Arguably, precision is the more convenient metric for the respondents, because it tells them how much they can trust the positive predictions they see. However, precision tends to be low for unpopular majors, because the “real yes-es” are rare (see proof in Appendix A). An extreme case is when a major has zero people who have it in their top 3, then precision is bound to be zero. Although precision is also biased, its bias at least reflects the data and some reality of the majors, and not the response set of the model. That is, there is not much the model can cheat with precision as with recall. Thus, we choose to report Precision-Gain, the stringent and less biased version of precision.

**F1.** It has been noted in the literature that there exists a trade-off between precision and recall. For a model to have a high recall, it needs to proactively say yes whenever there is a real signal. For a model to have high precision, it has to say yes only if it is very sure the signal is there. Thus, a leniently yes-saying model tends to have higher recall and lower precision, while a conservative model that tends to say no tends to have higher precision and lower recall. Hence, another commonly reported metric is F1, which is the harmonic mean of precision and recall weighted equally. However, raw F1 still rewards yes-saying very much, because, while precision is bound when data have been collected, recall can be cheated. To maximize F1, the model can still cheat by encouraging yes-saying. Still, F1 has the following advantages important for the current purpose:

1. Less cheatable than recall

2. Reflects recall, which is information that precision alone does not give

3. Clearer than accuracy about what the model is doing right (true positives or true negatives)

**F1-Gain and Precision-Gain.** Since **a**ll the performance metrics discussed above have expected values that are not 0, they could be seriously inflated if the model tends to predict the “yes” category (especially F1, and Recall), or when the “yes” category in the real data is a much more prevalent than the “no” category (inflating Precision). As such, a less biased, but stringent down-adjustment is given by the following formula:

, where the _m_ is the metric of interest, such as F1 or Precision, _E(m)_ is the expected value of the metric if the model’s predictions were independent of the data (i.e. if the model were blindly guessing). The formula can be seen as forcing the expected value to 0 and rescaling it to [- , 1], as long as _m_ falls in the range [0, 1]. On a side note, Gain is always smaller or equal to the oringal metric so long as m is < not 1 (see Appendix A for the proof), and can be negative, when the model did worse than chance.

Of all the metrics, we chose Precision-gain and F1-gain (see Table 4.5) roughly through a process of elimination. Accuracy, or [[SH1]](#_msocom_1) the overall correct guesses divided by all, does not distinguish between true positives from true negatives, so it is not reported. In the case of CMPA, it is more important, either, that when a major is in one’s Top 3 that it gets predicted “yes” (Recall), or, that when a major is predicted “yes” that is really is in one’s Top 3 in the data (Precision). This leads to Recall and/or Precision and/or F1. Recall is important but is easily inflatable by model predicting “yes” all the time, so reporting it alone is unsafe. Precision is important and less cheatable. F1, which is the harmonic mean of Precision and Recall, incorporates both information and is worth reporting too. Since F1 and Precision are still subject to non-zero chance guessing, Hu et al. took an (unconventionally) stringent view, to report Precision-gain and F1-gain, the version devoid of chance guessing of Precision and F1. The current study will continue with F1-gain and Precision-gain.

As is typical in Machine Learning literature, all the performance metrics reported are the generalization results to the “test”, as opposed to the “validation” dataset, explained next, in “K-fold cross validation”.

